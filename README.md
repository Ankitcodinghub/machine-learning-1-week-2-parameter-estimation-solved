# machine-learning-1-week-2-parameter-estimation-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 1 Week 2-Parameter-Estimation Solved](https://www.ankitcodinghub.com/product/machine-learning-1-week-2-parameter-estimation-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98746&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 1 Week 2-Parameter-Estimation Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
Exercise 1: Maximum-Likelihood Estimation (5 + 5 + 5 + 5 P)

We consider the problem of estimating using the maximum-likelihood approach the parameters , ⌘ &gt; 0 of the probability distribution:

p(x, y) = ⌘ex⌘y

supported on R2+. We consider a dataset D = ((x1,y1),…,(xN,yN)) composed of N independent draws

from this distribution.

<ol>
<li>(a) &nbsp;Show that x and y are independent.</li>
<li>(b) &nbsp;Derive a maximum likelihood estimator of the parameter based on D.</li>
<li>(c) &nbsp;Derive a maximum likelihood estimator of the parameter based on D under the constraint ⌘ = 1/.</li>
<li>(d) &nbsp;Derive a maximum likelihood estimator of the parameter based on D under the constraint ⌘ = 1 .Exercise 2: Maximum Likelihood vs. Bayes (5 + 10 + 15 P)
An unfair coin is tossed seven times and the event (head or tail) is recorded at each iteration. The observed sequence of events is

D = (x1, x2, . . . , x7) = (head, head, tail, tail, head, head, head).

We assume that all tosses x1, x2, . . . have been generated independently following the Bernoulli probability

distribution

P(x|✓)=⇢ ✓ if x=head 1✓ if x=tail,

where ✓ 2 [0, 1] is an unknown parameter.
</li>
</ol>
<ol>
<li>(a) &nbsp;State the likelihood function P(D|✓), that depends on the parameter ✓.</li>
<li>(b) &nbsp;Compute the maximum likelihood solution ✓ˆ, and evaluate for this parameter the probability that the next two tosses are “head”, that is, evaluate P (x8 = head , x9 = head | ✓ˆ).</li>
<li>(c) &nbsp;We now adopt a Bayesian view on this problem, where we assume a prior distribution for the parameter ✓ definedas: ⇢ 1 if 0✓1p(✓)= 0 else.

Compute the posterior distributioZn p(✓|D), and evaluate the probability that the next two tosses are head,

that is,

P(x8 = head, x9 = head | ✓)p(✓|D)d✓. Exercise 3: Convergence of Bayes Parameter Estimation (5 + 5 P)

We consider Section 3.4.1 of Duda et al., where the data is generated according to the univariate probability density p(x|μ) ⇠ N(μ,2), where 2 is known and where μ is unknown with prior distribution p(μ) ⇠ N(μ0,02). Having sampled a dataset D from the data-generating distribution, the posterior probability distribution over the unknown parameter μ becomes p(μ|D) ⇠ N(μn,n2), where
</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
1 n 1 μn n μ0 1Xn

</div>
</div>
<div class="layoutArea">
<div class="column">
n2 = 2 + 02 n2 = 2 μˆ n + 02 μˆ n = n x k . k=1

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
(a) Show that the variance of the posterior can be upper-bounded as n2  min(2/n , 02), that is, the variance of the posterior is contained both by the uncertainty of the data mean and of the prior.

(b) Show that the mean of the posterior can be lower- and upper-bounded as min(μˆn, μ0)  μn  max(μˆn, μ0), that is, the mean of the posterior distribution lies somewhere on the segment between the mean of the prior distribution and the sample mean.

Exercise 4: Programming (40 P)

Download the programming files on ISIS and follow the instructions.

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 2 (programming) [WiSe 2021/22] Machine Learning 1

</div>
</div>
<div class="layoutArea">
<div class="column">
Maximum Likelihood Parameter Estimation

In this first exercise, we would like to use the maximum-likelihood method to estimate the best parameter of a data density model p(x | θ) with respect to some dataset D = (x1, …, xN), and use that approach to build a classifier. Assuming the data is generated independently and identically distributed (iid.), the dataset likelihood is given by

</div>
</div>
<div class="layoutArea">
<div class="column">
and the maximum likelihood solution is then computed as

where the log term can also be expressed as a sum, i.e.

</div>
<div class="column">
N

p(D|θ) = ∏p(xk|θ)

k=1

θˆ = a r g m a x p ( D | θ ) θ

=argmax logp(D|θ) θ

N

logp(D | θ) = ∑ logp(xk | θ).

</div>
</div>
<div class="layoutArea">
<div class="column">
k=1

As a first step, we load some useful libraries for numerical computations and plotting.

</div>
</div>
<div class="layoutArea">
<div class="column">
In [1]:

import numpy

import matplotlib

%matplotlib inline

from matplotlib import pyplot as plt na = numpy.newaxis

We now consider the univariate data density model

</div>
</div>
<div class="layoutArea">
<div class="column">
p(x | θ) = 1 1

π 1 + (x − θ)2

</div>
</div>
<div class="layoutArea">
<div class="column">
also known as the Cauchy distribution with fixed parameter γ = 1, and with parameter θ unknown. Compared to the Gaussian distribution, the Cauchy distribution is heavy-tailed, and this can be useful to handle the presence of outliers in the data generation process. The probability density function is implemented below.

In [2]:

def pdf(X,THETA):

return (1.0 / numpy.pi) * (1.0 / (1+(X-THETA)**2))

Note that the function can be called with scalars or with numpy arrays, and if feeding arrays of different shape, numpy broadcasting rules will apply. Our

first step will be to implement a function that estimates the optimal parameter θˆ in the maximum likelihood sense for some dataset D. Task (10 P):

Implement a function that takes a dataset D as input (given as one-dimensional array of numbers) and a list of candidate parameters θ (also given as a one-dimensional array), and returns a one-dimensional array containing the log-likelihood w.r.t. the dataset D for each parameter θ.

In [3]:

def ll(D,THETA):

# ————————————–

# TODO: replace by your code

# ————————————– import solution; return solution.ll(D,THETA) # ————————————–

</div>
</div>
<div class="layoutArea">
<div class="column">
To test the method, we apply it to some dataset, and plot the log-likelihood for some plausible range of parameters θ.

</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
<pre>In [4]:
D = numpy.array([ 2.803, -1.563, -0.853,  2.212, -0.334,  2.503])
</pre>
<pre>THETA = numpy.linspace(-10,10,1001)
</pre>
plt.grid(True)

plt.plot(THETA,ll(D,THETA)) plt.xlabel(r’$\theta$’)

plt.ylabel(r’$\log p(\mathcal{D}|\theta)$’) plt.show()

Weobservethatthelikelihoodhastwopeaks:onearoundθ= −0.5andonearoundθ=2.However,thehighestpeakisthesecondone,hence,the second peak is retained as a maximum likelihood solution.

Building a Classifier

We now would like to use the maximum likelihood technique to build a classifier. We consider a labeled dataset where the data associated to the two classes are given by:

In [5]:

<pre>D1 = numpy.array([ 2.803, -1.563, -0.853,  2.212, -0.334,  2.503])
D2 = numpy.array([-4.510, -3.316, -3.050, -3.108, -2.315])
</pre>
To be able to classify new data points, we consider the discriminant function

g(x) = logP(x | θˆ 1) − logP(x | θˆ 2) + logP(ω1) − logP(ω2)

were the first two terms can be computed based on our maximum likelihood estimates, and where the last two terms are the prior probabilities. The

function g(x) produces the decision ω1 if g(x) &gt; 0 and ω2 if g(x) &lt; 0. We would like to implement a maximum-likelihood based classifier. Tasks (10 P):

Implement the function fit that receives as input a vector of candidate parameters θ and the dataset associated to each class, and produces the maximum likelihood parameter estimates. (Hint: from your function fit , you can call the function ll you have previously implemented.)

Implement the function predict that takes as input the prior probability for each class and a vector of points X on which to evaluate the discriminant function, and that outputs a vector containing the value of g for each point in X.

</div>
</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<div class="layoutArea">
<div class="column">
In [6]:

class MLClassifier:

def fit(self,THETA,D1,D2):

# ————————————– # TODO: replace by your code

# ————————————– import solution

<pre>        self.theta1,self.theta2 = solution.fit(THETA,D1,D2)
</pre>
<pre>        # --------------------------------------
</pre>
def predict(self,X,p1,p2):

# ————————————– # TODO: replace by your code

# ————————————– import solution

return solution.predict(self.theta1,self.theta2,X,p1,p2) # ————————————–

Once these two functions have been implemented, the maximum likelihood classifier can be applied to our labeled data, and the decision function it implements can be visualized.

<pre>In [7]:
X = numpy.linspace(-10,10,1001)
</pre>
plt.grid(True)

mlcl = MLClassifier()

<pre>mlcl.fit(THETA,D1,D2)
</pre>
<pre>plt.plot(X,mlcl.predict(X,0.5,0.5))
plt.plot(X,0*X,color='black',ls='dotted')
</pre>
<pre>plt.xlabel(r'$x$')
plt.ylabel(r'$g(x)$')
</pre>
for d1 in D1: plt.plot([d1,d1],[0,+0.5],color=’black’) for d2 in D2: plt.plot([d2,d2],[0,-0.5],color=’black’)

Here, we observe that the model essentially learns a threshold classifier with threshold approximately −0.5. However, we note that the threshold seems to be too high to properly classify the data. One reason for this is the fact that maximum likelihood estimate retains only the best parameter. Here, the model for the first class focuses mainly on the peak at x = 2 and treat examples x &lt; 0 as outliers, without considering the possibility that the peak at θ = 2 might actually be the outlier.

</div>
</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="section">
<div class="layoutArea">
<div class="column">
Bayes Parameter Estimation

Let us now bypass the computation of a maximum likelihood estimate of parameters and adopt instead a full Bayesian approach. We will consider the same data density model and datasets as in the maximum likelihood exercise but we include now a prior distribution over the parameters. Specifically, we set for both classes the prior distribution:

p(θ)=1 1

10π 1 + (θ/10)2

Given a dataset D, the posterior distribution for the unknown parameter θ can then be obtained from the Bayes rule: p(θ | D) = p(D | θ)p(θ)

</div>
</div>
<div class="layoutArea">
<div class="column">
The integration can be performed numerically using the trapezoidal rule.

Task (10 P):

</div>
</div>
<div class="layoutArea">
<div class="column">
∫ p(D | θ)p(θ)dθ

</div>
</div>
<div class="layoutArea">
<div class="column">
Implement the prior and posterior functions below. These function receive as input a vector of parameters θ (assumed to be sorted from smallest to largest, linearly spaced, and covering the range of values where most of the probability mass lies). The posterior function also receive a dataset D as input. Both functions return a vector containing the probability scores associated to each value of θ.

In [8]:

def prior(THETA):

# ————————————–

# TODO: replace by your code

# ————————————– import solution; return solution.prior(THETA) # ————————————–

def posterior(D,THETA):

# ————————————–

# TODO: replace by your code

# ————————————–

import solution; return solution.posterior(D,THETA) # ————————————–

To verify the implementation of the two functions, we apply them to the dataset D defined above and with a broad range of parameters θ. In [9]:

<pre>THETA = numpy.linspace(-100,100,10001)
</pre>
plt.grid(True)

plt.plot(THETA,numpy.log(prior(THETA)),label=r’$p(\theta)$’) plt.plot(THETA,numpy.log(posterior(D,THETA)),label=r’$p(\theta|\mathcal{D})$’) plt.legend(); plt.xlabel(r’$\theta$’); plt.show()

We observe that the posterior distribution is more concentrated to the specific values of the parameter that explain the dataset well. In particular, we observethesametwopeaksaroundθ= −0.5andθ=2observedinthemaximumlikelihoodexercise.

</div>
</div>
</div>
</div>
<div class="page" title="Page 7">
<div class="section">
<div class="layoutArea">
<div class="column">
Building a Classifier

We now would like to build a Bayes classifier based on the discriminant function

h(x) = logP(x | D1) − logP(x | D2) + logP(ω1) − logP(ω2)

where the dataset-conditioned densities are obtained from the original data density model and the parameter posterior as

p(x|Dj) = ∫p(x|θ)p(θ|Dj)dθ Implement a function fit that produces the parameter posteriors p(θ | D1) and p(θ | D2).

Implement a function predict computing the new discriminant function h based on the dataset-conditioned data densities. In [10]:

class BayesClassifier:

def fit(self,THETA,D1,D2):

# ————————————–

# TODO: replace by your code

# ————————————–

import solution

self.THETA,self.post1,self.post2 = solution.fitBayes(THETA,D1,D2) # ————————————–

def predict(self,X,p1,p2):

# ————————————– # TODO: replace by your code

# ————————————– import solution

return solution.predictBayes(self.THETA,self.post1,self.post2,X,p1,p2) # ————————————–

We note that the function predict is computationally more expensive than the one for maximum likelihood since it involves computing an integral for each point to be predicted.

However, the quality of the prediction also differs compared to that of the maximum likelihood method. In the plot below, we compare the ML and Bayes approaches.

<pre>In [11]:
X = numpy.linspace(-10,10,1001)
</pre>
<pre>bacl = BayesClassifier()
bacl.fit(THETA,D1,D2)
</pre>
plt.grid(True) plt.plot(X,mlcl.predict(X,0.5,0.5),label=’ML’) plt.plot(X,bacl.predict(X,0.5,0.5),label=’Bayes’)

<pre>plt.plot(X,0*X,color='black',ls='dotted')
plt.xlabel(r'$x$'); plt.ylabel(r'$g(x)$')
plt.legend()
</pre>
for d1 in D1: plt.plot([d1,d1],[0,+0.5],color=’black’) for d2 in D2: plt.plot([d2,d2],[0,-0.5],color=’black’)

</div>
</div>
<div class="layoutArea">
<div class="column">
Tasks (10 P):

</div>
</div>
</div>
</div>
<div class="page" title="Page 8">
<div class="section">
<div class="layoutArea">
<div class="column">
We observe that the Bayes classifier has generally lower output scores and its decision boundary has been noticeably shifted to the left, leading to better predictions for the current data. In this particular case, the difference between the two models can be explained by the fact that the Bayes one better integrates the possibility that negative examples for the first class are not necessarily outliers.

</div>
</div>
</div>
</div>
<div class="page" title="Page 9"></div>
<div class="page" title="Page 10"></div>
<div class="page" title="Page 11"></div>
<div class="page" title="Page 12"></div>
